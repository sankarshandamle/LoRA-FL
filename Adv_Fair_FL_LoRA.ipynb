{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "381edd20d723eefb",
      "metadata": {
        "id": "381edd20d723eefb"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "from sklearn.datasets import fetch_openml\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import pandas as pd\n",
        "import torch.nn.functional as F\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "from torch.utils.data import DataLoader, Subset\n",
        "from torch.utils.data import random_split\n",
        "from torch.utils.data import TensorDataset\n",
        "from copy import deepcopy\n",
        "from tqdm import tqdm\n",
        "\n",
        "from sklearn.datasets import fetch_openml\n",
        "\n",
        "import random\n",
        "import pickle\n",
        "import sys"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Mtr_RwEss89A",
      "metadata": {
        "id": "Mtr_RwEss89A"
      },
      "source": [
        "### Dataset Class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d821e6df",
      "metadata": {
        "id": "d821e6df"
      },
      "outputs": [],
      "source": [
        "### Dataset Class\n",
        "\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, data, targets, indices):\n",
        "        self.data = data\n",
        "        self.targets = targets\n",
        "        self.indicies = indices\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        # Retrieve data and target using the index\n",
        "        data_item = self.data[index]\n",
        "        target_item = self.targets[index]\n",
        "        index_item = self.indicies[index]\n",
        "        # Return data, target, and index\n",
        "        return data_item, target_item, index_item\n",
        "\n",
        "class IndexedSubset(Subset):\n",
        "    def __getitem__(self, idx):\n",
        "        data = self.dataset[self.indices[idx]]\n",
        "        return data, self.indices[idx]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Adevlu4FtAxZ",
      "metadata": {
        "id": "Adevlu4FtAxZ"
      },
      "source": [
        "# Neural Nets"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "T3dRR08JtBut",
      "metadata": {
        "id": "T3dRR08JtBut"
      },
      "source": [
        "## MLP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Ga1t40buXQnQ",
      "metadata": {
        "id": "Ga1t40buXQnQ"
      },
      "outputs": [],
      "source": [
        "### Standard MLP\n",
        "\n",
        "class Net(nn.Module):\n",
        "    def __init__(self, input_size: int) -> None:\n",
        "        super(Net, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_size, 64, bias=False)  # First hidden layer without bias\n",
        "        self.bn1 = nn.BatchNorm1d(64)                      # Batch normalization\n",
        "        self.fc2 = nn.Linear(64, 32, bias=False)           # Second hidden layer without bias\n",
        "        self.bn2 = nn.BatchNorm1d(32)\n",
        "        self.fc3 = nn.Linear(32, 1, bias=False)            # Output layer without bias\n",
        "        self.dropout = nn.Dropout(0.5)                     # Dropout for regularization\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        x = torch.relu(self.bn1(self.fc1(x)))  # Apply batch normalization after each linear layer\n",
        "        x = self.dropout(x)                    # Apply dropout after activation\n",
        "        x = torch.relu(self.bn2(self.fc2(x)))  # Apply batch normalization after each linear layer\n",
        "        x = self.dropout(x)                    # Apply dropout after activation\n",
        "        x = self.fc3(x)                        # Output layer\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "FJfTDnLftLRe",
      "metadata": {
        "id": "FJfTDnLftLRe"
      },
      "source": [
        "## LoRA Net"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "D2hFVgeffZk_",
      "metadata": {
        "id": "D2hFVgeffZk_"
      },
      "outputs": [],
      "source": [
        "### LoRA Adapter\n",
        "\n",
        "class LoRALinear(nn.Module):\n",
        "    def __init__(self, in_features: int, out_features: int, rank: int, alpha: float = 1.0):\n",
        "        super(LoRALinear, self).__init__()\n",
        "        self.base_layer = nn.Linear(in_features, out_features, bias=False)  # Main weight layer\n",
        "\n",
        "        # Freeze the base layer\n",
        "        for param in self.base_layer.parameters():\n",
        "            param.requires_grad = False  # Ensure base weights are not trainable\n",
        "\n",
        "        # LoRA-specific low-rank matrices\n",
        "        self.A = nn.Parameter(torch.randn(in_features, rank) * 1e-8)  # Small random initialization\n",
        "        self.B = nn.Parameter(torch.randn(rank, out_features) * 1e-8)\n",
        "\n",
        "        self.alpha = alpha  # Scaling factor\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.base_layer(x) + self.alpha * (x @ self.A @ self.B)\n",
        "\n",
        "class LoRANet(nn.Module):\n",
        "    def __init__(self, input_size: int, rank: int) -> None:\n",
        "        super(LoRANet, self).__init__()\n",
        "        self.fc1 = LoRALinear(input_size, 64, rank)  # First LoRA-enhanced hidden layer\n",
        "        self.bn1 = nn.BatchNorm1d(64)\n",
        "        self.fc2 = LoRALinear(64, 32, rank)          # Second LoRA-enhanced hidden layer\n",
        "        self.bn2 = nn.BatchNorm1d(32)\n",
        "        self.fc3 = LoRALinear(32, 1, rank)           # Output layer\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "\n",
        "    def normalize_lora_weights(self):\n",
        "        \"\"\"Normalizes LoRA weight matrices A and B to prevent numerical instability.\"\"\"\n",
        "        for name, module in self.named_modules():\n",
        "            if isinstance(module, LoRALinear):\n",
        "                if module.A.norm().item() > 0:  # Avoid division by zero\n",
        "                    module.A.data /= module.A.norm()\n",
        "                if module.B.norm().item() > 0:\n",
        "                    module.B.data /= module.B.norm()\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        # self.normalize_lora_weights()  # Ensure stability\n",
        "        x = torch.relu(self.bn1(self.fc1(x)))\n",
        "        x = self.dropout(x)\n",
        "        x = torch.relu(self.bn2(self.fc2(x)))\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc3(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "MwJUHOzfDj7q",
      "metadata": {
        "id": "MwJUHOzfDj7q"
      },
      "source": [
        "## Util that compares model mapping\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "NO06VcZiDnHT",
      "metadata": {
        "id": "NO06VcZiDnHT"
      },
      "outputs": [],
      "source": [
        "def compute_l2norm_diff(mlp_state_dict: dict, lora_state_dict: dict):\n",
        "    \"\"\"\n",
        "    Computes the L2 norm difference between the base weights of the standard MLP and LoRA-augmented MLP,\n",
        "    excluding 'A' and 'B' keys, and treating fc1.weight, fc2.weight, and fc3.weight as equivalent to\n",
        "    fc1.base_layer.weight, fc2.base_layer.weight, and fc3.base_layer.weight in the LoRA state dict.\n",
        "\n",
        "    :param mlp_state_dict: State dictionary of the standard MLP model\n",
        "    :param lora_state_dict: State dictionary of the LoRA-augmented MLP model\n",
        "    :return: Dictionary with L2 norm differences (layer-wise and FC total difference)\n",
        "    \"\"\"\n",
        "    l2_diffs = {}\n",
        "    total_fc_diff = 0.0\n",
        "\n",
        "    # Match corresponding layers\n",
        "    for key in mlp_state_dict:\n",
        "            # Skip 'A' and 'B' keys in LoRA state_dict\n",
        "            if 'A' in key or 'B' in key:\n",
        "                continue\n",
        "\n",
        "            mlp_weight = mlp_state_dict[key]\n",
        "\n",
        "            # For fc1.weight, fc2.weight, and fc3.weight, match them with base_layer weights in LoRA\n",
        "            if 'fc1.weight' in key:\n",
        "                lora_weight = lora_state_dict['fc1.base_layer.weight']\n",
        "            elif 'fc2.weight' in key:\n",
        "                lora_weight = lora_state_dict['fc2.base_layer.weight']\n",
        "            elif 'fc3.weight' in key:\n",
        "                lora_weight = lora_state_dict['fc3.base_layer.weight']\n",
        "            else:\n",
        "                lora_weight = lora_state_dict[key]\n",
        "\n",
        "\n",
        "            mlp_weight = mlp_weight.float()\n",
        "            lora_weight = lora_weight.float()\n",
        "\n",
        "            l2_diff = torch.norm(lora_weight - mlp_weight, p=2).item()\n",
        "            l2_diffs[key] = l2_diff\n",
        "\n",
        "            # Aggregate FC differences separately\n",
        "            if \"fc\" in key and \"weight\" in key:\n",
        "                  total_fc_diff += l2_diff\n",
        "\n",
        "    # Add total FC weight difference\n",
        "    l2_diffs[\"total_fc_diff\"] = total_fc_diff\n",
        "\n",
        "    return l2_diffs\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "VvhQwVm_P3tK",
      "metadata": {
        "id": "VvhQwVm_P3tK"
      },
      "outputs": [],
      "source": [
        "def compute_l2norm_diff_between_mlps(mlp1_state_dict: dict, mlp2_state_dict: dict):\n",
        "    \"\"\"\n",
        "    Computes the L2 norm difference between the base weights of the standard MLP and LoRA-augmented MLP,\n",
        "    including both fully connected (FC) and batch normalization (BN) layers.\n",
        "\n",
        "    :param mlp_state_dict: State dictionary of the standard MLP model\n",
        "    :param lora_state_dict: State dictionary of the LoRA-augmented MLP model\n",
        "    :return: Dictionary with L2 norm differences (layer-wise and FC total difference)\n",
        "    \"\"\"\n",
        "    l2_diffs = {}\n",
        "\n",
        "    # Match corresponding layers\n",
        "    for key in mlp1_state_dict:\n",
        "        # Ensure the tensors are floating-point before computing the L2 norm\n",
        "        l2_diffs[key] = torch.norm(mlp1_state_dict[key].float() - mlp2_state_dict[key].float(), p=2).item()\n",
        "\n",
        "    return l2_diffs\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "kdHbY54KufDl",
      "metadata": {
        "id": "kdHbY54KufDl"
      },
      "source": [
        "# Fairness Measures"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "JU-rhKmNeKmm",
      "metadata": {
        "id": "JU-rhKmNeKmm"
      },
      "source": [
        "### EO"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ATXB9wNXeMhw",
      "metadata": {
        "id": "ATXB9wNXeMhw"
      },
      "outputs": [],
      "source": [
        "def compute_equalized_odds(y_true_priv, y_pred_priv, y_true_unpriv, y_pred_unpriv):\n",
        "    \"\"\"\n",
        "    Compute Equalized Odds as the difference in True Positive Rates (TPR)\n",
        "    and False Positive Rates (FPR) between privileged and unprivileged groups.\n",
        "    \"\"\"\n",
        "    def tpr_fpr(y_true, y_pred):\n",
        "        tp = sum(1 for yt, yp in zip(y_true, y_pred) if yt == 1 and yp == 1)\n",
        "        fn = sum(1 for yt, yp in zip(y_true, y_pred) if yt == 1 and yp == 0)\n",
        "        fp = sum(1 for yt, yp in zip(y_true, y_pred) if yt == 0 and yp == 1)\n",
        "        tn = sum(1 for yt, yp in zip(y_true, y_pred) if yt == 0 and yp == 0)\n",
        "\n",
        "        tpr = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
        "        fpr = fp / (fp + tn) if (fp + tn) > 0 else 0\n",
        "        return tpr, fpr\n",
        "\n",
        "    tpr_priv, fpr_priv = tpr_fpr(y_true_priv, y_pred_priv)\n",
        "    tpr_unpriv, fpr_unpriv = tpr_fpr(y_true_unpriv, y_pred_unpriv)\n",
        "\n",
        "    return abs(tpr_unpriv - tpr_priv) + abs(fpr_unpriv - fpr_priv)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### DP"
      ],
      "metadata": {
        "id": "VfHgiTgoU4_P"
      },
      "id": "VfHgiTgoU4_P"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "sdgCg5auhEgQ",
      "metadata": {
        "id": "sdgCg5auhEgQ"
      },
      "outputs": [],
      "source": [
        "def compute_demographic_parity(preds_priv, preds_unpriv):\n",
        "    \"\"\"\n",
        "    Compute Demographic Parity as the absolute difference in positive classification rates\n",
        "    between the privileged and unprivileged groups.\n",
        "    \"\"\"\n",
        "    rate_priv = sum(preds_priv) / len(preds_priv) if len(preds_priv) > 0 else 0\n",
        "    rate_unpriv = sum(preds_unpriv) / len(preds_unpriv) if len(preds_unpriv) > 0 else 0\n",
        "\n",
        "    return abs(rate_unpriv - rate_priv)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "KHelVvwbtPCK",
      "metadata": {
        "id": "KHelVvwbtPCK"
      },
      "source": [
        "# **Attack**\n",
        "\n",
        "Custom Loss Functions"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Q9thaHZotTik",
      "metadata": {
        "id": "Q9thaHZotTik"
      },
      "source": [
        "## L_UF: Custom functions that add bias"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fxIBJK3CbMG8",
      "metadata": {
        "id": "fxIBJK3CbMG8"
      },
      "outputs": [],
      "source": [
        "class CustomCrossEntropyLoss(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CustomCrossEntropyLoss, self).__init__()\n",
        "        self.weight = 1.0\n",
        "\n",
        "    def forward(self, logits, targets, sensitive_feature):\n",
        "        \"\"\"\n",
        "        Computes the cross-entropy loss.\n",
        "\n",
        "        Args:\n",
        "            logits: Tensor of shape (N, C) where N is the batch size and C is the number of classes.\n",
        "                    These are raw model outputs before softmax.\n",
        "            targets: Tensor of shape (N,) where each value is the correct class index (0 ≤ target < C).\n",
        "\n",
        "        Returns:\n",
        "            A scalar loss value.\n",
        "        \"\"\"\n",
        "        # Sigmoid activation to convert logits to probabilities\n",
        "\n",
        "        logits = logits.squeeze()\n",
        "        probs = torch.sigmoid(logits)\n",
        "\n",
        "        p = 0\n",
        "        m = 0\n",
        "\n",
        "        p_loss = 0\n",
        "        m_loss = 0\n",
        "\n",
        "        for i in range(len(sensitive_feature)):\n",
        "            if sensitive_feature[i] == 'Male':\n",
        "                p+=1\n",
        "                p_loss += -(targets[i] * torch.log(probs[i]) + (1 - targets[i]) * torch.log(1 - probs[i]))\n",
        "            else:\n",
        "                m+=1\n",
        "                m_loss += -(targets[i] * torch.log(probs[i]) + (1 - targets[i]) * torch.log(1 - probs[i]))\n",
        "\n",
        "        # Maximize disparity: flip the loss order and use a weighted factor\n",
        "        loss = (m_loss / m) - self.weight * (p_loss / p)  # Minimize this to increase disparity\n",
        "        # print(loss)\n",
        "        if torch.isnan(loss).any() or torch.isinf(loss).any():\n",
        "            print(\"⚠️ NaN detected in L_UF!\")\n",
        "            sys.exit(0)\n",
        "\n",
        "        return loss\n",
        "\n",
        "\n",
        "class LabelFlipCrossEntropyLoss(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(LabelFlipCrossEntropyLoss, self).__init__()\n",
        "\n",
        "    def forward(self, logits, targets, sensitive_feature):\n",
        "        \"\"\"\n",
        "        Computes the cross-entropy loss.\n",
        "\n",
        "        Args:\n",
        "            logits: Tensor of shape (N, C) where N is the batch size and C is the number of classes.\n",
        "                    These are raw model outputs before softmax.\n",
        "            targets: Tensor of shape (N,) where each value is the correct class index (0 ≤ target < C).\n",
        "\n",
        "        Returns:\n",
        "            A scalar loss value.\n",
        "        \"\"\"\n",
        "        # Sigmoid activation to convert logits to probabilities\n",
        "\n",
        "        logits = logits.squeeze()\n",
        "        probs = torch.sigmoid(logits)\n",
        "\n",
        "        p = 0\n",
        "        m = 0\n",
        "\n",
        "        p_loss = 0\n",
        "        m_loss = 0\n",
        "\n",
        "        for i in range(len(sensitive_feature)):\n",
        "            if sensitive_feature[i] == 'Male':\n",
        "                targets[i] = 1.0\n",
        "            else:\n",
        "                targets[i] = 0.0\n",
        "\n",
        "        loss = -(targets * torch.log(probs) + (1 - targets) * torch.log(1 - probs)).mean()\n",
        "\n",
        "\n",
        "        return loss\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Increase Bias"
      ],
      "metadata": {
        "id": "umgfKO9YVKq_"
      },
      "id": "umgfKO9YVKq_"
    },
    {
      "cell_type": "code",
      "source": [
        "class DisparateImpactMaximizerLoss(nn.Module):\n",
        "    def __init__(self, weight=0.5, epsilon=1e-5):\n",
        "        super(DisparateImpactMaximizerLoss, self).__init__()\n",
        "        self.weight = weight\n",
        "        self.epsilon = epsilon\n",
        "\n",
        "    def forward(self, logits, targets, sensitive_feature):\n",
        "        logits = logits.squeeze()\n",
        "\n",
        "        # Apply sigmoid and clamp to avoid log(0) or log(1)\n",
        "        probs = torch.sigmoid(logits)\n",
        "        probs = torch.clamp(probs, min=self.epsilon, max=1 - self.epsilon)\n",
        "\n",
        "        # Convert sensitive_feature ('Male', 'Female') to 0 and 1\n",
        "        sensitive_feature_tensor = torch.tensor([1 if x == 'Male' else 0 for x in sensitive_feature], dtype=torch.float)\n",
        "\n",
        "        # Create masks for sensitive feature (Male = 1, Female = 0)\n",
        "        male_mask = sensitive_feature_tensor  # 1 for Male, 0 for Female\n",
        "        female_mask = 1 - male_mask  # 1 for Female, 0 for Male\n",
        "\n",
        "        # Calculate log loss\n",
        "        log_loss = -(targets * torch.log(probs) + (1 - targets) * torch.log(1 - probs))\n",
        "\n",
        "        # Calculate loss for each group (and prevent NaN issues from empty groups)\n",
        "        male_mask_sum = male_mask.sum() + self.epsilon\n",
        "        female_mask_sum = female_mask.sum() + self.epsilon\n",
        "\n",
        "\n",
        "\n",
        "        #### DemP ####\n",
        "        male_p  = (male_mask * probs).sum() / male_mask_sum\n",
        "        female_p = (female_mask * probs).sum() / female_mask_sum\n",
        "\n",
        "        demp = torch.abs(male_p - female_p)\n",
        "\n",
        "        #### EO #####\n",
        "        male_positive_sum = (male_mask * targets).sum()\n",
        "        female_positive_sum = (female_mask * targets).sum()\n",
        "        male_fnr = (male_mask * (1 - probs) * targets).sum() / male_positive_sum\n",
        "        female_fnr = (female_mask * (1 - probs) * targets).sum() / female_positive_sum\n",
        "\n",
        "        male_negative_sum = (male_mask * (1 - targets)).sum()\n",
        "        female_negative_sum = (female_mask * (1 - targets)).sum()\n",
        "        male_fpr = (male_mask * probs * (1 - targets)).sum() / male_negative_sum\n",
        "        female_fpr = (female_mask * probs * (1 - targets)).sum() / female_negative_sum\n",
        "        eo = torch.max(torch.abs(male_fnr - female_fnr), torch.abs(male_fpr - female_fpr))\n",
        "\n",
        "\n",
        "        male_loss = (male_mask * log_loss).sum() / male_mask_sum\n",
        "        female_loss = (female_mask * log_loss).sum() / female_mask_sum\n",
        "\n",
        "\n",
        "        male_loss = (male_mask * log_loss).sum() / male_mask_sum\n",
        "        female_loss = (female_mask * log_loss).sum() / female_mask_sum\n",
        "\n",
        "        # Calculate absolute difference and normalize between 0 and 1\n",
        "        abs_diff = torch.abs(male_loss - female_loss)\n",
        "\n",
        "        # Normalize the difference to range [0, 1] using min_max_range\n",
        "        min_range, max_range = torch.min(male_loss, female_loss), torch.max(male_loss, female_loss)\n",
        "        normalized_diff = (abs_diff - min_range) / (max_range - min_range)\n",
        "\n",
        "        # Clamp the normalized difference to ensure it stays between 0 and 1\n",
        "        normalized_diff = torch.clamp(normalized_diff, min=0.0, max=1.0)\n",
        "\n",
        "        # Inverse the normalized difference and calculate the final loss\n",
        "        # loss = 1 / (normalized_diff + self.epsilon)\n",
        "\n",
        "        loss = -torch.abs(eo)\n",
        "\n",
        "        # Debugging: Check for NaN or Inf values\n",
        "        if torch.isnan(loss).any() or torch.isinf(loss).any():\n",
        "            print(\"⚠️ NaN detected in loss!\")\n",
        "            print(f\"Loss: {loss}\")\n",
        "            print(f\"Probs: {probs}\")\n",
        "            print(f\"Male Mask Sum: {male_mask_sum}\")\n",
        "            print(f\"Female Mask Sum: {female_mask_sum}\")\n",
        "            print(f\"male_loss: {male_loss}\")\n",
        "            print(f\"female_loss: {female_loss}\")\n",
        "            print(f\"log_loss: {log_loss}\")\n",
        "            sys.exit(0)\n",
        "\n",
        "        return loss"
      ],
      "metadata": {
        "id": "1BBIMK2YVdxv"
      },
      "id": "1BBIMK2YVdxv",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "l4Bph7emujai",
      "metadata": {
        "id": "l4Bph7emujai"
      },
      "source": [
        "# **Main Attack Loss**\n",
        "\n",
        "We compute L_UF and a regualizer loss with the adapters and local (benign) model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "CZn684YXbJCe",
      "metadata": {
        "id": "CZn684YXbJCe"
      },
      "outputs": [],
      "source": [
        "class CustomLowRankLoss(nn.Module):\n",
        "    def __init__(self):\n",
        "        \"\"\"\n",
        "        Initializes the loss function with a given low-rank factorization rank.\n",
        "        \"\"\"\n",
        "        super(CustomLowRankLoss, self).__init__()\n",
        "        self.lam = 0.5\n",
        "        self.epsilon = 1e-8\n",
        "\n",
        "    def compute_dynamic_weights(self, loss1, loss2, focus_factor=0.5, scaling=None):\n",
        "        \"\"\"\n",
        "        Computes dynamic weights for loss1 and loss2.\n",
        "\n",
        "        Parameters:\n",
        "        - loss1, loss2: Loss values (normalized to sum to 1).\n",
        "        - focus_factor: How much importance to give loss2 (0 → only loss1, 1 → only loss2).\n",
        "        - scaling: 'softmax' (default), 'inverse', or 'exponential'.\n",
        "\n",
        "        Returns:\n",
        "        - Dynamic weights for loss1 and loss2.\n",
        "        \"\"\"\n",
        "        total = loss1 + loss2 + self.epsilon\n",
        "\n",
        "        if scaling == 'softmax':\n",
        "            T = 0.1  # Lower T makes distribution sharper\n",
        "            losses = torch.tensor([loss1, loss2]) / total\n",
        "            weights = torch.nn.functional.softmax(losses / T, dim=0)\n",
        "\n",
        "        else:\n",
        "            weights = torch.tensor([loss1, loss2]) / total\n",
        "\n",
        "        # Ensure no loss is neglected\n",
        "        weight_sum = weights.sum()\n",
        "\n",
        "        # Adjust weights to ensure they are balanced and not ignored\n",
        "        adjusted_weights = weights / weight_sum\n",
        "\n",
        "        # Dynamically adjust weight based on focus_factor\n",
        "        weighted_loss1 = (1 - focus_factor) * adjusted_weights[0].item()\n",
        "        weighted_loss2 = focus_factor * adjusted_weights[1].item()\n",
        "\n",
        "        # Combine the weighted losses\n",
        "        lam = weighted_loss1 / (weighted_loss1 + weighted_loss2)\n",
        "\n",
        "        # Ensure Lambda doesn't get too small or too large\n",
        "        lam = max(min(lam, 1.0 - self.epsilon), self.epsilon)  # Clamped between epsilon and 1-epsilon\n",
        "\n",
        "        return lam\n",
        "\n",
        "    def forward(self, theta_g, theta_i, theta_lora, logits, targets, sensitive_labels):\n",
        "        \"\"\"\n",
        "        Computes the loss for low-rank adaptation.\n",
        "\n",
        "        Args:\n",
        "            theta_g: Global model parameters (k-1)th.\n",
        "            theta_i: Benign Local model parameters (k)th.\n",
        "            theta_g - theta_i : \\Delta \\theta for the kth round\n",
        "            theta_lora: Adversarial LoRA-enhanced model parameters.\n",
        "            logits: Tensor of shape (N, C), raw model outputs before softmax.\n",
        "            targets: Tensor of shape (N,), correct class indices.\n",
        "            sensitive_labels: List of sensitive labels for each data point.\n",
        "\n",
        "        Returns:\n",
        "            A scalar loss value combining cross-entropy and low-rank regularization.\n",
        "        \"\"\"\n",
        "\n",
        "        \"\"\"\n",
        "        Loss1: AB^T - theta_i - theta_g\n",
        "        \"\"\"\n",
        "\n",
        "        \"\"\"\n",
        "        \\delta theta = theta_i - theta_g\n",
        "        \\delta theta = AB^T\n",
        "        output: theta_g + AB^T\n",
        "        \"\"\"\n",
        "\n",
        "        ## first compute theta_i - theta_g\n",
        "        added_params = {\n",
        "            name.split('.')[0]: param1 - param2\n",
        "            for ((name, param1), (_, param2)) in zip(theta_g.named_parameters(), theta_i.named_parameters())\n",
        "            if 'fc' in name  # Check for 'fc' to include fc1, fc2, and fc3 layers\n",
        "        }\n",
        "\n",
        "        # added_params = {\n",
        "        #     name.split('.')[0]: param1\n",
        "        #     for (name, param1) in theta_i.named_parameters()\n",
        "        #     if 'fc' in name  # Check for 'fc' to include fc1, fc2, and fc3 layers\n",
        "        # }\n",
        "\n",
        "        ## Do AB^T - added_params\n",
        "        # Compute AB^T for each LoRALinear layer in l1\n",
        "        lora_params = {}\n",
        "        for name, module in theta_lora.named_modules():\n",
        "            if isinstance(module, LoRALinear):\n",
        "                lora_params[name] = module.A @ module.B  # Compute AB^T\n",
        "\n",
        "        for name, ab in lora_params.items():\n",
        "            if torch.isnan(ab).any() or torch.isinf(ab).any():\n",
        "                print(f\"NaN or Inf detected in {name}\")\n",
        "                sys.exit(0)\n",
        "\n",
        "        for name, ab in added_params.items():\n",
        "            if torch.isnan(ab).any() or torch.isinf(ab).any():\n",
        "                print(f\"NaN or Inf detected in AddedParams {name}\")\n",
        "                sys.exit(0)\n",
        "\n",
        "\n",
        "        loss1 = sum(torch.norm(ab.T - added_params[name], p=2) for name, ab in lora_params.items())\n",
        "        # total_norm = sum(torch.norm(ab.T, p=2) for ab in lora_params.values()) + 1e-6  # Avoid divide by zero\n",
        "        # loss1 /= total_norm\n",
        "\n",
        "        if torch.isnan(loss1).any():\n",
        "            print(\"⚠️ NaN detected in loss1!\")\n",
        "            sys.exit(0)\n",
        "\n",
        "        \"\"\"\n",
        "        Loss2: Unfairness Loss\n",
        "        \"\"\"\n",
        "        loss_uf = DisparateImpactMaximizerLoss()\n",
        "        loss2 = loss_uf(logits, targets, sensitive_labels)\n",
        "\n",
        "        \"\"\"\n",
        "        \"\"\"\n",
        "        # temp_loss1 = loss1.clone()\n",
        "        # temp_loss2 = loss2.clone()\n",
        "        # self.lam = self.compute_dynamic_weights(temp_loss1, temp_loss2, 0.999)\n",
        "\n",
        "        # total = loss1 + loss2 + self.epsilon\n",
        "        # loss1 = loss1 / total\n",
        "        # loss2 = loss2 / total\n",
        "        overall_loss = self.lam * loss1 + (1 - self.lam) * loss2\n",
        "\n",
        "        # print(f\"loss1: {loss1}, loss2: {loss2}, Lambda: {self.lam}\")\n",
        "\n",
        "        return loss1\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomTwoPhaseLowRankLoss(nn.Module):\n",
        "    def __init__(self):\n",
        "        \"\"\"\n",
        "        Initializes the loss function with a given low-rank factorization rank.\n",
        "        \"\"\"\n",
        "        super(CustomTwoPhaseLowRankLoss, self).__init__()\n",
        "        self.lam = 0.5\n",
        "        self.epsilon = 1e-8\n",
        "\n",
        "\n",
        "    def Phase1Regularizer(self, theta_g, theta_i, theta_lora, logits, targets, sensitive_labels):\n",
        "        \"\"\"\n",
        "        Computes the loss for low-rank adaptation.\n",
        "\n",
        "        Args:\n",
        "            theta_g: Global model parameters (k-1)th.\n",
        "            theta_i: Benign Local model parameters (k)th.\n",
        "            theta_g - theta_i : \\Delta \\theta for the kth round\n",
        "            theta_lora: Adversarial LoRA-enhanced model parameters.\n",
        "            logits: Tensor of shape (N, C), raw model outputs before softmax.\n",
        "            targets: Tensor of shape (N,), correct class indices.\n",
        "            sensitive_labels: List of sensitive labels for each data point.\n",
        "\n",
        "        Returns:\n",
        "            A scalar loss value combining cross-entropy and low-rank regularization.\n",
        "        \"\"\"\n",
        "\n",
        "        \"\"\"\n",
        "        Loss1: AB^T - theta_i - theta_g\n",
        "        \"\"\"\n",
        "\n",
        "        \"\"\"\n",
        "        \\delta theta = theta_i - theta_g\n",
        "        \\delta theta = AB^T\n",
        "        output: theta_g + AB^T\n",
        "        \"\"\"\n",
        "\n",
        "        ## first compute theta_i - theta_g\n",
        "        added_params = {\n",
        "            name.split('.')[0]: param1 - param2\n",
        "            for ((name, param1), (_, param2)) in zip(theta_g.named_parameters(), theta_i.named_parameters())\n",
        "            if 'fc' in name  # Check for 'fc' to include fc1, fc2, and fc3 layers\n",
        "        }\n",
        "\n",
        "        ## Do AB^T - added_params\n",
        "        # Compute AB^T for each LoRALinear layer in l1\n",
        "        lora_params = {}\n",
        "        for name, module in theta_lora.named_modules():\n",
        "            if isinstance(module, LoRALinear):\n",
        "                lora_params[name] = module.A @ module.B  # Compute AB^T\n",
        "\n",
        "        for name, ab in lora_params.items():\n",
        "            if torch.isnan(ab).any() or torch.isinf(ab).any():\n",
        "                print(f\"NaN or Inf detected in {name}\")\n",
        "                sys.exit(0)\n",
        "\n",
        "        for name, ab in added_params.items():\n",
        "            if torch.isnan(ab).any() or torch.isinf(ab).any():\n",
        "                print(f\"NaN or Inf detected in AddedParams {name}\")\n",
        "                sys.exit(0)\n",
        "\n",
        "\n",
        "        loss1 = sum(torch.norm(ab.T - added_params[name], p=2) for name, ab in lora_params.items())\n",
        "\n",
        "        if torch.isnan(loss1).any():\n",
        "            print(\"⚠️ NaN detected in loss1!\")\n",
        "            sys.exit(0)\n",
        "\n",
        "        return loss1\n",
        "\n",
        "    def Phase2FairAttack(self, logits, targets, sensitive_labels):\n",
        "\n",
        "        loss_uf = DisparateImpactMaximizerLoss()\n",
        "        loss2 = loss_uf(logits, targets, sensitive_labels)\n",
        "\n",
        "        return loss2\n",
        "\n"
      ],
      "metadata": {
        "id": "_19CR_Q6WZ7q"
      },
      "id": "_19CR_Q6WZ7q",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "Mui5vVB9uxqL",
      "metadata": {
        "id": "Mui5vVB9uxqL"
      },
      "source": [
        "# Training!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "-e9fRpX3X48z",
      "metadata": {
        "id": "-e9fRpX3X48z"
      },
      "outputs": [],
      "source": [
        "## Handles the training pipeline\n",
        "\n",
        "HONEST_LR = 5e-4\n",
        "ADV_LR = 5e-4\n",
        "\n",
        "class Device:\n",
        "    def __init__(self, id, dataset, train_indices, test_indices, raw_frame, sensitive_feature = 'sex', adversarial = False, iid=True) -> None:\n",
        "\n",
        "        input_size = len(dataset[0][0])\n",
        "        self.id = id\n",
        "\n",
        "        self.model = Net(input_size=input_size)\n",
        "        self.global_model = Net(input_size=input_size)\n",
        "        # self.model.to(torch.device(\"cuda\"))\n",
        "\n",
        "        if(adversarial):\n",
        "            self.lora_model = LoRANet(input_size=input_size, rank=4)\n",
        "            self.criterion_honest = nn.BCEWithLogitsLoss()\n",
        "            self.criterion_adv = CustomTwoPhaseLowRankLoss() #\n",
        "            self.optimizer_lora_phase1 = torch.optim.AdamW(\n",
        "                                        [p for p in self.lora_model.parameters() if p.requires_grad], lr=ADV_LR\n",
        "                                  )\n",
        "            # first freeze A adapter\n",
        "            self.freezeAadapter()\n",
        "            self.optimizer_lora_phase2 = torch.optim.AdamW(\n",
        "                                        [p for p in self.lora_model.parameters() if p.requires_grad], lr=ADV_LR\n",
        "                                  )\n",
        "            # unfreeze A adapter\n",
        "            self.unfreezeAadapter()\n",
        "\n",
        "        else:\n",
        "            self.criterion = nn.BCEWithLogitsLoss()\n",
        "\n",
        "        self.optimizer = optim.AdamW(self.model.parameters(), lr=HONEST_LR)\n",
        "\n",
        "        self.train_indices = train_indices\n",
        "        self.test_indices = test_indices\n",
        "\n",
        "        self.train_set = Subset(dataset, train_indices)\n",
        "        self.test_set = Subset(dataset, test_indices)\n",
        "\n",
        "        self.sensitive_feature = sensitive_feature\n",
        "        self.raw_frame = raw_frame\n",
        "\n",
        "        self.adversarial = adversarial\n",
        "\n",
        "        self.train_loader = DataLoader(self.train_set, batch_size=512, shuffle=False)\n",
        "        self.test_loader = DataLoader(self.test_set, batch_size=512, shuffle=False)\n",
        "        self.iid = iid\n",
        "\n",
        "\n",
        "    def train(self, train_loader = None, num_epochs = 5):\n",
        "        for device_epoch in range(num_epochs):\n",
        "            self.model.train()\n",
        "            losses = []\n",
        "            for inputs, labels, indices in self.train_loader:\n",
        "                self.optimizer.zero_grad()\n",
        "\n",
        "                # inputs, train_labels = inputs.cuda(), train_labels.cuda()  # add this line\n",
        "                labels = labels.float()\n",
        "                # print(indices)\n",
        "\n",
        "                sensitive_label = self.raw_frame.loc[indices.tolist(), 'sex'].tolist()\n",
        "\n",
        "                outputs = self.model(inputs)\n",
        "                outputs = outputs.squeeze()\n",
        "                labels = labels.view_as(outputs)\n",
        "                if(self.adversarial):\n",
        "                    loss = self.criterion_honest(outputs, labels)\n",
        "                else:\n",
        "                    loss = self.criterion(outputs, labels)\n",
        "                loss.backward()\n",
        "                self.optimizer.step()\n",
        "                losses.append(loss.item())\n",
        "            # print(f\"loss: {np.array(losses).mean()}\")\n",
        "\n",
        "    def train_adversarial(self, theta_g, theta_i, train_loader = None, num_epochs = 2):\n",
        "\n",
        "        for device_epoch in range(num_epochs):\n",
        "\n",
        "            self.lora_model.train()\n",
        "            losses_phase1 = []\n",
        "            losses_phase2 = []\n",
        "\n",
        "            for inputs, labels, indices in self.train_loader:\n",
        "\n",
        "                self.optimizer_lora_phase1.zero_grad()\n",
        "                self.optimizer_lora_phase2.zero_grad()\n",
        "\n",
        "                \"\"\"\n",
        "                Phase 1 : Regularizer\n",
        "                \"\"\"\n",
        "\n",
        "                # inputs, train_labels = inputs.cuda(), train_labels.cuda()  # add this line\n",
        "                labels = labels.float()\n",
        "                # print(indices)\n",
        "\n",
        "                sensitive_label = self.raw_frame.loc[indices.tolist(), 'sex'].tolist()\n",
        "\n",
        "                outputs = self.lora_model(inputs)\n",
        "                outputs = outputs.squeeze()\n",
        "                labels = labels.view_as(outputs)\n",
        "\n",
        "                if torch.isnan(outputs).any():\n",
        "                    print(\"⚠️ NaN detected in logits!\")\n",
        "                    sys.exit(0)\n",
        "\n",
        "                loss_phase1 = self.criterion_adv.Phase1Regularizer(theta_g, theta_i, self.lora_model, outputs, labels, sensitive_label)\n",
        "                if torch.isnan(loss_phase1).any():\n",
        "                    print(\"⚠️ NaN detected in loss!\")\n",
        "                    sys.exit(0)\n",
        "\n",
        "\n",
        "                loss_phase1.backward()\n",
        "                self.optimizer_lora_phase1.step()\n",
        "                losses_phase1.append(loss_phase1.item())\n",
        "\n",
        "                \"\"\"\n",
        "                Phase 2: Fairness Attack\n",
        "                \"\"\"\n",
        "                outputs = self.lora_model(inputs)\n",
        "                outputs = outputs.squeeze()\n",
        "                labels = labels.view_as(outputs)\n",
        "\n",
        "                loss_phase2 = self.criterion_adv.Phase2FairAttack(outputs, labels, sensitive_label)\n",
        "                loss_phase2.backward()\n",
        "                self.optimizer_lora_phase2.step()\n",
        "                losses_phase2.append(loss_phase2.item())\n",
        "                if torch.isnan(loss_phase2).any():\n",
        "                    print(\"⚠️ NaN detected in loss!\")\n",
        "                    sys.exit(0)\n",
        "\n",
        "\n",
        "    def validate(self, test_loader=None, flag=False, verbose=False):\n",
        "        # Set the model to evaluation mode\n",
        "        self.model.eval()\n",
        "\n",
        "        # Use the default test_loader if none is provided\n",
        "        if test_loader is None:\n",
        "            test_loader = self.test_loader\n",
        "\n",
        "        total = 0\n",
        "        correct = 0\n",
        "        losses = []\n",
        "\n",
        "        pr_y1_unpriv = 0\n",
        "        pr_y1_priv = 0\n",
        "        total_unpriv = 0\n",
        "        total_priv = 0\n",
        "\n",
        "        y_true_priv, y_pred_priv = [], []\n",
        "        y_true_unpriv, y_pred_unpriv = [], []\n",
        "\n",
        "        preds_priv, preds_unpriv = [], []  # Needed for Demographic Parity\n",
        "\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for inputs, labels, indices in self.train_loader:\n",
        "\n",
        "                # Move data to GPU if needed (uncomment if using CUDA)\n",
        "                # inputs, labels = inputs.cuda(), labels.cuda()\n",
        "\n",
        "                # Convert labels to float for BCEWithLogitsLoss\n",
        "                labels = labels.float()\n",
        "\n",
        "                sensitive_label = self.raw_frame.loc[indices.tolist(), 'sex'].tolist()\n",
        "\n",
        "\n",
        "                # Forward pass\n",
        "                if self.adversarial and flag:\n",
        "                    outputs = self.lora_model(inputs)\n",
        "                else:\n",
        "                    outputs = self.model(inputs)\n",
        "\n",
        "                # Adjust shapes\n",
        "                outputs = outputs.squeeze()\n",
        "                labels = labels.view_as(outputs)\n",
        "\n",
        "                # Compute loss\n",
        "                if(self.adversarial):\n",
        "                    loss = self.criterion_honest(outputs, labels)\n",
        "                else:\n",
        "                    loss = self.criterion(outputs, labels)\n",
        "                losses.append(loss.item())\n",
        "\n",
        "                # Apply sigmoid to logits and threshold at 0.5 for predictions\n",
        "                predictions = torch.sigmoid(outputs) >= 0.5\n",
        "\n",
        "                for i in range(len(predictions)):\n",
        "                    if sensitive_label[i] == 'Male':\n",
        "                        total_priv += 1\n",
        "                        if predictions[i] == labels[i]:\n",
        "                            pr_y1_priv += 1\n",
        "                        y_true_priv.append(labels[i].item())\n",
        "                        y_pred_priv.append(predictions[i].item())\n",
        "                        preds_priv.append(predictions[i].item())\n",
        "\n",
        "\n",
        "                    else:\n",
        "                        total_unpriv += 1\n",
        "                        if predictions[i] == labels[i]:\n",
        "                            pr_y1_unpriv += 1\n",
        "                        y_true_unpriv.append(labels[i].item())\n",
        "                        y_pred_unpriv.append(predictions[i].item())\n",
        "\n",
        "                        preds_unpriv.append(predictions[i].item())\n",
        "\n",
        "                # Update total and correct counts\n",
        "                total += labels.size(0)\n",
        "                correct += (predictions == labels).sum().item()\n",
        "\n",
        "        # Calculate accuracy\n",
        "        accuracy = 100 * correct / total\n",
        "        disparate_impact = pr_y1_unpriv/total_unpriv / (pr_y1_priv/total_priv)\n",
        "        equalized_odds = compute_equalized_odds(y_true_priv, y_pred_priv, y_true_unpriv, y_pred_unpriv)\n",
        "        demographic_parity = compute_demographic_parity(preds_priv, preds_unpriv)\n",
        "\n",
        "\n",
        "        # Verbose logging\n",
        "        if verbose:\n",
        "            print(f\"Disparate Impact: {disparate_impact:.2f}\")\n",
        "            print(f\"Accuracy: {accuracy:.2f}%\")\n",
        "            print(f\"Average Loss: {sum(losses) / len(losses):.4f}\")\n",
        "\n",
        "        return sum(losses) / len(losses), accuracy, disparate_impact, equalized_odds, demographic_parity\n",
        "\n",
        "\n",
        "    def get_model_params(self):\n",
        "        # if self.adversarial:\n",
        "        #     return convert_lora_to_standard(self.lora_model.state_dict(), 1.0)\n",
        "        return self.model.state_dict()\n",
        "\n",
        "    def set_global_model(self, current_global_model):\n",
        "        self.global_model.load_state_dict(current_global_model)\n",
        "        # print(missing, unexpected)\n",
        "\n",
        "    def update_model(self, new_state_dict):\n",
        "        self.model.load_state_dict(new_state_dict, strict=False)\n",
        "        # print(missing, unexpected)\n",
        "\n",
        "    def transfer_mlp_to_lora(self, new_state_dict):\n",
        "        \"\"\"\n",
        "        Transfers the weights from a standard MLP model to the LoRANet model.\n",
        "\n",
        "        - Copies fc1, fc2, fc3 weights from the MLP to `fc1.base_layer`, `fc2.base_layer`, and `fc3.base_layer` in LoRANet.\n",
        "        - Leaves LoRA adapter parameters (A, B) unchanged.\n",
        "\n",
        "        Args:\n",
        "            lora_model (LoRANet): The LoRA model to be updated.\n",
        "            mlp_model (nn.Module): The standard MLP model with matching architecture.\n",
        "        \"\"\"\n",
        "        lora_state_dict = self.lora_model.state_dict()\n",
        "\n",
        "        for name, param in new_state_dict.items():\n",
        "            if \"fc\" in name and \"weight\" in name:  # Only transfer fully connected layers\n",
        "                lora_name = f\"{name.split('.')[0]}.base_layer.{name.split('.')[1]}\"\n",
        "                # print(lora_name)\n",
        "                lora_state_dict[lora_name] = param  # Copy the MLP weights\n",
        "\n",
        "            elif \"bn\" in name:  # Transfer batch normalization layers\n",
        "                lora_state_dict[name] = param  # Copy BN weights\n",
        "\n",
        "        # Load the modified state dict into the LoRA model\n",
        "        missing, unexpected = self.lora_model.load_state_dict(lora_state_dict, strict=True)\n",
        "        # print(missing, unexpected)\n",
        "\n",
        "    def convert_lora_to_standard(self, alpha):\n",
        "        \"\"\"\n",
        "        Converts the LoRA state dict to a standard MLP state dict by merging the base weights and the low-rank adaptations.\n",
        "\n",
        "        Args:\n",
        "            lora_state_dict (dict): The state dictionary from a LoRA-based model.\n",
        "            alpha (float): The scaling factor for the LoRA low-rank adaptation.\n",
        "\n",
        "        Returns:\n",
        "            dict: The converted standard state dictionary.\n",
        "        \"\"\"\n",
        "        # Initialize a dictionary to hold the converted state_dict\n",
        "        standard_state_dict = {}\n",
        "        lora_state_dict = deepcopy(self.lora_model.state_dict())\n",
        "\n",
        "        # Convert each LoRA layer to a standard MLP layer by merging A @ B^T with base weights\n",
        "        # For the LoRALinear layers (fc1, fc2, fc3)\n",
        "        for layer in ['fc1', 'fc2', 'fc3']:\n",
        "            # Compute the standard layer weight: theta' = theta + A @ B^T\n",
        "            standard_state_dict[f\"{layer}.weight\"] = (\n",
        "                lora_state_dict[f\"{layer}.base_layer.weight\"] +\n",
        "                (alpha * lora_state_dict[f\"{layer}.A\"] @ lora_state_dict[f\"{layer}.B\"]).T\n",
        "            )\n",
        "\n",
        "\n",
        "        # Handle batch normalization layers\n",
        "        for i in range(1, 3):\n",
        "            standard_state_dict[f\"bn{i}.weight\"] = lora_state_dict[f\"bn{i}.weight\"]\n",
        "            standard_state_dict[f\"bn{i}.bias\"] = lora_state_dict[f\"bn{i}.bias\"]\n",
        "            standard_state_dict[f\"bn{i}.running_mean\"] = lora_state_dict[f\"bn{i}.running_mean\"]\n",
        "            standard_state_dict[f\"bn{i}.running_var\"] = lora_state_dict[f\"bn{i}.running_var\"]\n",
        "            standard_state_dict[f\"bn{i}.num_batches_tracked\"] = lora_state_dict[f\"bn{i}.num_batches_tracked\"]\n",
        "\n",
        "        return standard_state_dict\n",
        "\n",
        "\n",
        "    def add_adapters(self):\n",
        "        # temp_model = deepcopy(self.model)\n",
        "        self.model.load_state_dict(self.convert_lora_to_standard(1.0), strict=True)\n",
        "        # total_norm = sum(torch.norm(param1 - param2, p=2) ** 2\n",
        "                    #  for (name, param1), (_, param2) in zip(temp_model.named_parameters(), self.model.named_parameters()))\n",
        "        # print(f\"Total norm: {total_norm}\")\n",
        "\n",
        "\n",
        "    def freezeAadapter(self):\n",
        "\n",
        "        for name, param in self.lora_model.state_dict().items():\n",
        "            if 'A' in name:  # Look for A adapters\n",
        "                param.requires_grad = False  # Freeze the parameter\n",
        "\n",
        "            if 'B' in name:\n",
        "                param.requires_grad = True\n",
        "\n",
        "    def unfreezeAadapter(self):\n",
        "\n",
        "        for name, param in self.lora_model.state_dict().items():\n",
        "            if 'B' in name:  # Look for B adapters\n",
        "                param.requires_grad = True  # Freeze the parameter\n",
        "\n",
        "            if 'A' in name:\n",
        "                param.requires_grad = True\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ZoVxD8PYu0Iv",
      "metadata": {
        "id": "ZoVxD8PYu0Iv"
      },
      "source": [
        "# FL Aggregation"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "-SQ4wkb-u22f",
      "metadata": {
        "id": "-SQ4wkb-u22f"
      },
      "source": [
        "### FedAvg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7abb8fc2cb492835",
      "metadata": {
        "id": "7abb8fc2cb492835"
      },
      "outputs": [],
      "source": [
        "def average_state_dicts(state_dicts):\n",
        "    \"\"\"\n",
        "    Averages the parameters of the state_dicts from all client models.\n",
        "\n",
        "    :param state_dicts: a list of state_dicts from client models\n",
        "    :return: the averaged state_dict\n",
        "    \"\"\"\n",
        "    # Initialize a new state_dict with the keys from the first state_dict\n",
        "    avg_state_dict = {key: torch.zeros_like(value) for key, value in state_dicts[0].items()}\n",
        "\n",
        "    # Sum all the state_dicts\n",
        "    for state_dict in state_dicts:\n",
        "        for key, value in state_dict.items():\n",
        "            avg_state_dict[key] += value\n",
        "\n",
        "    # Divide by the number of state_dicts to get the average\n",
        "    for key in avg_state_dict.keys():\n",
        "        avg_state_dict[key] = avg_state_dict[key] / len(state_dicts)\n",
        "\n",
        "    return avg_state_dict"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "F13TToP55IG9",
      "metadata": {
        "id": "F13TToP55IG9"
      },
      "source": [
        "## Krum"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b7UjrDIb5Jrn",
      "metadata": {
        "id": "b7UjrDIb5Jrn"
      },
      "outputs": [],
      "source": [
        "def krum_state_dicts(state_dicts, f, num_selected=10):\n",
        "    \"\"\"\n",
        "    Implements the KRUM aggregation algorithm with multiple selections.\n",
        "\n",
        "    :param state_dicts: a list of state_dicts from client models\n",
        "    :param f: upper bound on the number of Byzantine (malicious) clients\n",
        "    :param num_selected: number of state_dicts to select\n",
        "    :return: a list of selected state_dicts\n",
        "    \"\"\"\n",
        "    n = len(state_dicts)\n",
        "    m = n - f - 2  # Number of closest distances to consider\n",
        "    if m < 1:\n",
        "        raise ValueError(\"n - f - 2 must be at least 1\")\n",
        "\n",
        "    # Flatten each state_dict into a vector\n",
        "    param_vectors = []\n",
        "    for state_dict in state_dicts:\n",
        "        params = []\n",
        "        for key in sorted(state_dict.keys()):\n",
        "            if \"bn\" in str(key):\n",
        "                continue\n",
        "            # print(key)\n",
        "            params.append(state_dict[key].flatten())\n",
        "        param_vector = torch.cat(params)\n",
        "        param_vectors.append(param_vector)\n",
        "\n",
        "    # Compute pairwise distances\n",
        "    distances = torch.zeros(n, n)\n",
        "    for i in range(n):\n",
        "        for j in range(i + 1, n):\n",
        "            dist = torch.norm(param_vectors[i] - param_vectors[j]) ** 2\n",
        "            distances[i][j] = dist\n",
        "            distances[j][i] = dist  # Symmetric\n",
        "\n",
        "    # Compute KRUM scores\n",
        "    krum_scores = []\n",
        "    for i in range(n):\n",
        "        dists = torch.cat((distances[i, :i], distances[i, i+1:]))\n",
        "        m_closest_dists, _ = torch.topk(dists, k=m, largest=False)\n",
        "        score = torch.sum(m_closest_dists)\n",
        "        krum_scores.append(score.item())\n",
        "\n",
        "\n",
        "    print(krum_scores)\n",
        "\n",
        "    # Select the indices of the top `num_selected` clients with the lowest scores\n",
        "    selected_indices = torch.topk(torch.tensor(krum_scores), k=num_selected, largest=False).indices.tolist()\n",
        "    selected_state_dicts = [state_dicts[i] for i in selected_indices]\n",
        "\n",
        "    print(\"Selected indices:\", selected_indices)\n",
        "\n",
        "    return average_state_dicts(selected_state_dicts)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "FW7riIVtu4x5",
      "metadata": {
        "id": "FW7riIVtu4x5"
      },
      "source": [
        "# Runner!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6c2bd4a171126d8b",
      "metadata": {
        "id": "6c2bd4a171126d8b"
      },
      "outputs": [],
      "source": [
        "SEED = 0\n",
        "torch.manual_seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "random.seed(SEED)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9690c266",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9690c266",
        "outputId": "86db7dc5-ac76-48c8-9a50-fd1a7ffa9935"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0 0 1 ... 0 0 1]\n"
          ]
        }
      ],
      "source": [
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "# Fetch the Adult dataset\n",
        "adult = fetch_openml(name='adult', version=2, as_frame=True)\n",
        "df = adult.frame\n",
        "\n",
        "# Separate features (X) and target (y)\n",
        "X = df.drop('class', axis=1)\n",
        "y = df['class']\n",
        "\n",
        "# Extract feature column names\n",
        "feature_names = X.columns.tolist()\n",
        "\n",
        "# Replace '?' with NaN and drop missing values\n",
        "X = X.replace('?', np.nan)\n",
        "X = X.dropna()\n",
        "y = y[X.index]  # Align target variable\n",
        "\n",
        "categorical_cols = X.select_dtypes(include=['category']).columns\n",
        "numerical_cols = X.select_dtypes(include=['int64', 'float64']).columns\n",
        "\n",
        "train_cols = categorical_cols.drop('sex')\n",
        "sensitive_cols = categorical_cols.drop(['workclass', 'education', 'marital-status', 'occupation',\n",
        "       'relationship', 'race', 'native-country'])\n",
        "\n",
        "# Define the preprocessing steps\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', StandardScaler(), numerical_cols),\n",
        "        ('cat', OneHotEncoder(sparse_output=False), train_cols)\n",
        "    ])\n",
        "\n",
        "# Apply the preprocessing pipeline\n",
        "X_processed = preprocessor.fit_transform(X)\n",
        "feature_names = preprocessor.get_feature_names_out()\n",
        "\n",
        "# Encode the target variable\n",
        "le = LabelEncoder()\n",
        "y_processed = le.fit_transform(y)\n",
        "\n",
        "print(y_processed)\n",
        "\n",
        "X_dataset = torch.tensor(X_processed, dtype=torch.float32)\n",
        "y_dataset = torch.tensor(y_processed, dtype=torch.long)\n",
        "\n",
        "indices = X.index.tolist()\n",
        "\n",
        "base_dataset = CustomDataset(X_dataset, y_dataset, indices)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ed72c049",
      "metadata": {
        "id": "ed72c049"
      },
      "outputs": [],
      "source": [
        "NO_DEVICES = 10\n",
        "POINTS_PER_DEVICE = 4000\n",
        "TEST_RATIO = 0.25\n",
        "PERCENT_ADVERSARIAL = 0.40\n",
        "f = int(PERCENT_ADVERSARIAL * NO_DEVICES)\n",
        "\n",
        "IID = True\n",
        "ALPHA = 0.5\n",
        "\n",
        "NO_EPOCHS = 10\n",
        "ADV_EPOCHS = 10\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "69b6be4ad8b13746",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "69b6be4ad8b13746",
        "outputId": "1528d031-860e-415b-ec8b-c3f7645eaf92"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0, 1, 2, 3]\n"
          ]
        }
      ],
      "source": [
        "devices = []\n",
        "\n",
        "for agent_id in range(NO_DEVICES):\n",
        "    train_start = int(agent_id * POINTS_PER_DEVICE)\n",
        "    train_end = int((agent_id+1) * POINTS_PER_DEVICE - POINTS_PER_DEVICE * (TEST_RATIO))\n",
        "    test_start = train_end\n",
        "    test_end = int((agent_id+1) * POINTS_PER_DEVICE)\n",
        "\n",
        "\n",
        "    train_indices = indices[train_start:train_end]\n",
        "    test_indices = indices[test_start:test_end]\n",
        "\n",
        "\n",
        "    adversarial = False\n",
        "    if agent_id < int(NO_DEVICES * PERCENT_ADVERSARIAL):\n",
        "        adversarial = True\n",
        "\n",
        "    device = Device(agent_id, base_dataset, train_indices, test_indices, X, sensitive_feature='sex', adversarial=adversarial, iid=IID)\n",
        "    devices.append(device)\n",
        "\n",
        "adv_ids = []\n",
        "for device in devices:\n",
        "    if device.adversarial:\n",
        "        adv_ids.append(device.id)\n",
        "print(adv_ids)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a692862a119c7e1",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a692862a119c7e1",
        "outputId": "b02e7b60-59f6-486b-c127-e86b46db56fb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  2%|▏         | 1/50 [00:12<10:09, 12.44s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average accuracy:  75.56333333333333\n",
            "Average disparate impact:  1.2937926076503634\n",
            "Average EQ Odds:  0.01914530321550192\n",
            "Average DP:  0.0023364390781731538\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  4%|▍         | 2/50 [00:25<10:03, 12.58s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average accuracy:  82.58333333333333\n",
            "Average disparate impact:  1.1616858943585942\n",
            "Average EQ Odds:  0.3292255940505511\n",
            "Average DP:  0.26338792430942326\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  6%|▌         | 3/50 [00:37<09:40, 12.34s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average accuracy:  82.95333333333333\n",
            "Average disparate impact:  1.1543903156356776\n",
            "Average EQ Odds:  0.2730283622510198\n",
            "Average DP:  0.2471421941181179\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  8%|▊         | 4/50 [00:49<09:30, 12.41s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average accuracy:  83.11333333333334\n",
            "Average disparate impact:  1.1591854577666314\n",
            "Average EQ Odds:  0.2793898090200535\n",
            "Average DP:  0.2514048835864161\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 10%|█         | 5/50 [01:02<09:18, 12.42s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average accuracy:  83.16666666666667\n",
            "Average disparate impact:  1.1616466956755975\n",
            "Average EQ Odds:  0.2781381413779958\n",
            "Average DP:  0.25167719046463244\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 12%|█▏        | 6/50 [01:14<09:03, 12.35s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average accuracy:  83.33000000000001\n",
            "Average disparate impact:  1.1616797488783785\n",
            "Average EQ Odds:  0.3090764017438355\n",
            "Average DP:  0.2597598134433194\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 14%|█▍        | 7/50 [01:26<08:47, 12.27s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average accuracy:  83.19333333333333\n",
            "Average disparate impact:  1.166420815773833\n",
            "Average EQ Odds:  0.3276361153616018\n",
            "Average DP:  0.2686749097295139\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 16%|█▌        | 8/50 [01:39<08:47, 12.56s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average accuracy:  83.07666666666667\n",
            "Average disparate impact:  1.169808920388392\n",
            "Average EQ Odds:  0.35187491343021776\n",
            "Average DP:  0.27926506715614074\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 18%|█▊        | 9/50 [01:53<08:46, 12.83s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average accuracy:  82.99666666666667\n",
            "Average disparate impact:  1.1743907561424205\n",
            "Average EQ Odds:  0.39379696566206024\n",
            "Average DP:  0.29379588561824493\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 20%|██        | 10/50 [02:06<08:35, 12.89s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average accuracy:  82.76333333333334\n",
            "Average disparate impact:  1.178775185753656\n",
            "Average EQ Odds:  0.43585523031125933\n",
            "Average DP:  0.30794418795358036\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 22%|██▏       | 11/50 [02:18<08:18, 12.78s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average accuracy:  82.51\n",
            "Average disparate impact:  1.1854992828267548\n",
            "Average EQ Odds:  0.4722360715620832\n",
            "Average DP:  0.32088547420846714\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 24%|██▍       | 12/50 [02:31<08:06, 12.80s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average accuracy:  81.85\n",
            "Average disparate impact:  1.1993616200853077\n",
            "Average EQ Odds:  0.5466978561356587\n",
            "Average DP:  0.34773850469951445\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 26%|██▌       | 13/50 [02:45<08:04, 13.10s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average accuracy:  81.78666666666666\n",
            "Average disparate impact:  1.201925892452894\n",
            "Average EQ Odds:  0.5907208064101452\n",
            "Average DP:  0.3584713483635691\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 28%|██▊       | 14/50 [02:57<07:43, 12.88s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average accuracy:  81.66999999999999\n",
            "Average disparate impact:  1.2045943599610696\n",
            "Average EQ Odds:  0.62558086948583\n",
            "Average DP:  0.3667624748335748\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 30%|███       | 15/50 [03:10<07:27, 12.80s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average accuracy:  81.63333333333334\n",
            "Average disparate impact:  1.2062924360875331\n",
            "Average EQ Odds:  0.6342030417004799\n",
            "Average DP:  0.3701892967603097\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 32%|███▏      | 16/50 [03:22<07:13, 12.76s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average accuracy:  81.57000000000001\n",
            "Average disparate impact:  1.2075453065480355\n",
            "Average EQ Odds:  0.6297882564093004\n",
            "Average DP:  0.3706358862269341\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 34%|███▍      | 17/50 [03:37<07:14, 13.17s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average accuracy:  81.37333333333332\n",
            "Average disparate impact:  1.2140991912118895\n",
            "Average EQ Odds:  0.6297205463780158\n",
            "Average DP:  0.3763730676078241\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 36%|███▌      | 18/50 [03:50<07:08, 13.40s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average accuracy:  81.45\n",
            "Average disparate impact:  1.214840873875437\n",
            "Average EQ Odds:  0.6195919412240165\n",
            "Average DP:  0.3754710310012959\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 38%|███▊      | 19/50 [04:02<06:41, 12.95s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average accuracy:  81.36666666666667\n",
            "Average disparate impact:  1.2168041098499545\n",
            "Average EQ Odds:  0.629697635337476\n",
            "Average DP:  0.38209991759151973\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 40%|████      | 20/50 [04:15<06:23, 12.79s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average accuracy:  81.4\n",
            "Average disparate impact:  1.215793152133739\n",
            "Average EQ Odds:  0.618536687619877\n",
            "Average DP:  0.3774906787715218\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 42%|████▏     | 21/50 [04:27<06:04, 12.57s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average accuracy:  81.37333333333332\n",
            "Average disparate impact:  1.2183614060332908\n",
            "Average EQ Odds:  0.6277301592575227\n",
            "Average DP:  0.3820227367914891\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 44%|████▍     | 22/50 [04:39<05:47, 12.40s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average accuracy:  81.49000000000001\n",
            "Average disparate impact:  1.2158264296998251\n",
            "Average EQ Odds:  0.6030617198117465\n",
            "Average DP:  0.37525636087509484\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 46%|████▌     | 23/50 [04:51<05:35, 12.41s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average accuracy:  81.49666666666666\n",
            "Average disparate impact:  1.2154581920823873\n",
            "Average EQ Odds:  0.6035046677411272\n",
            "Average DP:  0.3756882253952639\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 48%|████▊     | 24/50 [05:05<05:32, 12.78s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average accuracy:  81.67333333333332\n",
            "Average disparate impact:  1.213213953229499\n",
            "Average EQ Odds:  0.595814759968541\n",
            "Average DP:  0.372913677728426\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 50%|█████     | 25/50 [05:19<05:26, 13.07s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average accuracy:  81.66333333333334\n",
            "Average disparate impact:  1.2134546707650762\n",
            "Average EQ Odds:  0.5915396880859658\n",
            "Average DP:  0.37195197803371294\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 52%|█████▏    | 26/50 [05:32<05:12, 13.02s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average accuracy:  81.66666666666667\n",
            "Average disparate impact:  1.212227868278095\n",
            "Average EQ Odds:  0.5848594466707561\n",
            "Average DP:  0.3709868074390822\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 54%|█████▍    | 27/50 [05:47<05:14, 13.66s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average accuracy:  81.65666666666667\n",
            "Average disparate impact:  1.2131092317074519\n",
            "Average EQ Odds:  0.6026876055409909\n",
            "Average DP:  0.3740099033841019\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 56%|█████▌    | 28/50 [05:59<04:52, 13.31s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average accuracy:  81.79666666666667\n",
            "Average disparate impact:  1.2113342323921936\n",
            "Average EQ Odds:  0.592412053090414\n",
            "Average DP:  0.37116272345133405\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 58%|█████▊    | 29/50 [06:10<04:24, 12.58s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average accuracy:  81.69\n",
            "Average disparate impact:  1.2118713225647557\n",
            "Average EQ Odds:  0.58839540691465\n",
            "Average DP:  0.3704377874097604\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 60%|██████    | 30/50 [06:22<04:08, 12.41s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average accuracy:  81.85999999999999\n",
            "Average disparate impact:  1.2085307946016575\n",
            "Average EQ Odds:  0.5788879143890674\n",
            "Average DP:  0.3666562727691851\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 62%|██████▏   | 31/50 [06:35<03:55, 12.41s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average accuracy:  81.94666666666667\n",
            "Average disparate impact:  1.2064810407219322\n",
            "Average EQ Odds:  0.571103235349576\n",
            "Average DP:  0.3647001941521911\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 64%|██████▍   | 32/50 [06:47<03:42, 12.34s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average accuracy:  81.84\n",
            "Average disparate impact:  1.209204251058423\n",
            "Average EQ Odds:  0.5729181701939019\n",
            "Average DP:  0.3665384566249764\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 66%|██████▌   | 33/50 [06:58<03:24, 12.00s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average accuracy:  81.90333333333334\n",
            "Average disparate impact:  1.209228693127593\n",
            "Average EQ Odds:  0.5650886280180418\n",
            "Average DP:  0.3651210347901235\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 68%|██████▊   | 34/50 [07:10<03:12, 12.06s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average accuracy:  81.97666666666666\n",
            "Average disparate impact:  1.2066455298150924\n",
            "Average EQ Odds:  0.5599764815525654\n",
            "Average DP:  0.3627334282613145\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 70%|███████   | 35/50 [07:21<02:57, 11.81s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average accuracy:  82.01\n",
            "Average disparate impact:  1.2048088311627858\n",
            "Average EQ Odds:  0.5737233676743305\n",
            "Average DP:  0.3635925857971348\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 72%|███████▏  | 36/50 [07:33<02:45, 11.82s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average accuracy:  81.96333333333334\n",
            "Average disparate impact:  1.2057749633442874\n",
            "Average EQ Odds:  0.569955018946694\n",
            "Average DP:  0.3641735812402595\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 74%|███████▍  | 37/50 [07:45<02:34, 11.89s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average accuracy:  82.05333333333334\n",
            "Average disparate impact:  1.2051399878974662\n",
            "Average EQ Odds:  0.5557662248071926\n",
            "Average DP:  0.3600929286028426\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 76%|███████▌  | 38/50 [07:57<02:20, 11.71s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average accuracy:  82.06333333333333\n",
            "Average disparate impact:  1.2044854927283635\n",
            "Average EQ Odds:  0.5363169205568595\n",
            "Average DP:  0.35632775143563955\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 78%|███████▊  | 39/50 [08:09<02:09, 11.81s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average accuracy:  82.1\n",
            "Average disparate impact:  1.2052861681682192\n",
            "Average EQ Odds:  0.5310741994904011\n",
            "Average DP:  0.3560721491850808\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 80%|████████  | 40/50 [08:20<01:58, 11.85s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average accuracy:  82.25666666666669\n",
            "Average disparate impact:  1.2020637962574363\n",
            "Average EQ Odds:  0.5314160801938713\n",
            "Average DP:  0.35419267074165933\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 82%|████████▏ | 41/50 [08:33<01:49, 12.16s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average accuracy:  82.31333333333333\n",
            "Average disparate impact:  1.2013942051807118\n",
            "Average EQ Odds:  0.5372046447510556\n",
            "Average DP:  0.35417312163205106\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 84%|████████▍ | 42/50 [08:45<01:36, 12.00s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average accuracy:  82.25333333333333\n",
            "Average disparate impact:  1.202140368743507\n",
            "Average EQ Odds:  0.5370770778463926\n",
            "Average DP:  0.35546123823312686\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 86%|████████▌ | 43/50 [08:57<01:23, 11.98s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average accuracy:  82.34\n",
            "Average disparate impact:  1.1999280480372865\n",
            "Average EQ Odds:  0.5209803550964035\n",
            "Average DP:  0.35030270731928076\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 88%|████████▊ | 44/50 [09:10<01:12, 12.15s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average accuracy:  82.41000000000001\n",
            "Average disparate impact:  1.1981149736475252\n",
            "Average EQ Odds:  0.5432302240468065\n",
            "Average DP:  0.3527623698480672\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 90%|█████████ | 45/50 [09:23<01:02, 12.50s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average accuracy:  82.47333333333333\n",
            "Average disparate impact:  1.1979289752380455\n",
            "Average EQ Odds:  0.5260030877686765\n",
            "Average DP:  0.34897781464460176\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 92%|█████████▏| 46/50 [09:35<00:50, 12.53s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average accuracy:  82.35666666666665\n",
            "Average disparate impact:  1.199764717025986\n",
            "Average EQ Odds:  0.5228871697374142\n",
            "Average DP:  0.34956249667649597\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 94%|█████████▍| 47/50 [09:48<00:37, 12.52s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average accuracy:  82.28333333333333\n",
            "Average disparate impact:  1.2033672000635034\n",
            "Average EQ Odds:  0.5213735182416537\n",
            "Average DP:  0.3520580978547629\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 96%|█████████▌| 48/50 [10:00<00:24, 12.46s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average accuracy:  82.34\n",
            "Average disparate impact:  1.2015946615707631\n",
            "Average EQ Odds:  0.525121626790039\n",
            "Average DP:  0.3517504197363988\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 98%|█████████▊| 49/50 [10:12<00:12, 12.29s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average accuracy:  82.41333333333333\n",
            "Average disparate impact:  1.2004988096977256\n",
            "Average EQ Odds:  0.5130981812612023\n",
            "Average DP:  0.34830551193037607\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 50/50 [10:25<00:00, 12.51s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average accuracy:  82.47333333333333\n",
            "Average disparate impact:  1.1982701424764106\n",
            "Average EQ Odds:  0.5127806176432114\n",
            "Average DP:  0.34759796466212645\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "NO_COM_ROUNDS = 50\n",
        "AGGREGATOR = 0\n",
        "aggregator_results = {}\n",
        "\n",
        "\n",
        "for com_round in tqdm(range(0, NO_COM_ROUNDS)):\n",
        "    # print(f'\\nCommunication round: {com_round}')\n",
        "    client_state_dicts = []\n",
        "    losses = []\n",
        "    accuracies = []\n",
        "    disparate_impacts = []\n",
        "    eq_odds = []\n",
        "    dp = []\n",
        "\n",
        "    # Train local models\n",
        "    for device in devices:\n",
        "        device.train(num_epochs=NO_EPOCHS)\n",
        "        if device.adversarial:\n",
        "            temp_i = deepcopy(device.model)\n",
        "            temp_g = deepcopy(device.global_model)\n",
        "            device.train_adversarial(temp_g, temp_i, num_epochs=ADV_EPOCHS)\n",
        "            # _, _, _ = device.validate(flag=True, verbose=True)\n",
        "            device.add_adapters() ##\n",
        "            # print(device.global_model.state_dict())\n",
        "            # print(compute_l2norm_diff_between_mlps(device.model.state_dict(), device.global_model.state_dict()))\n",
        "            # _, _, _ = device.validate(verbose=True)\n",
        "        client_state_dicts.append(device.get_model_params())\n",
        "        # sys.exit(0)\n",
        "\n",
        "\n",
        "    client_update = average_state_dicts(client_state_dicts)\n",
        "    # client_update = krum_state_dicts(client_state_dicts, f) # aggregated global model\n",
        "\n",
        "    # model update\n",
        "    for device in devices:\n",
        "        device.set_global_model(client_update)\n",
        "        device.update_model(client_update)\n",
        "        if device.adversarial:\n",
        "            device.transfer_mlp_to_lora(client_update)  ##\n",
        "            # print(f\"L2norm: {compute_l2norm_diff(client_update, device.lora_model.state_dict())}\")\n",
        "        loss, accuracy, disparate_impact, eq_odd, dp_x = device.validate(verbose=False)\n",
        "        losses.append(loss)\n",
        "        accuracies.append(accuracy)\n",
        "        disparate_impacts.append(disparate_impact)\n",
        "        eq_odds.append(eq_odd)\n",
        "        dp.append(dp_x)\n",
        "\n",
        "    # for i in range(10):\n",
        "    #     print(compute_l2norm_diff_between_mlps(devices[i].global_model.state_dict(), client_update))\n",
        "\n",
        "    # print(\"Average loss: \", sum(losses) / len(losses))\n",
        "    print(\"Average accuracy: \", sum(accuracies) / len(accuracies))\n",
        "    print(\"Average disparate impact: \", sum(disparate_impacts) / len(disparate_impacts))\n",
        "    print(\"Average EQ Odds: \", sum(eq_odds) / len(eq_odds))\n",
        "    print(\"Average DP: \", sum(dp) / len(dp))\n",
        "    print(\"\\n\")\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "collapsed_sections": [
        "T3dRR08JtBut",
        "MwJUHOzfDj7q",
        "kdHbY54KufDl"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}